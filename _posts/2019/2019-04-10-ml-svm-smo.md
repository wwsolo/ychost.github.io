---
layout: post
title: SMO 算法剖析及实现
categories: [机器学习]
description: 1996年，John Platt 发布一个称为 SMO（Sequential Minimal Optimization，序列最小优化）的强大算法，用于训练 SVM，该算法的核心思想是将原问题分解成多个小问题分别进行优化求解。
keywords: SVM, SMO
tags: [SMO, SVM, 机器学习]
excerpt: "1996年，John Platt 发布一个称为 SMO（Sequential Minimal Optimization，序列最小优化）的强大算法，用于训练 SVM，该算法的核心思想是将原问题分解成多个小问题分别进行优化求解。"
readMins: 30
---

### 优化目标

SVM 的优化目标函数：

$$
\begin{split}
    \mathop{min}\limits_{\vec{\alpha}} \Psi(\vec{\alpha}) = \begin{cases}
        \underbrace{ min }_{\alpha}  \frac{1}{2} \sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_i\alpha_jy_iy_jK(x_i,x_j) - \sum\limits_{i=1}^{N}\alpha_i\\
        s.t. \; \sum\limits_{i=1}^{N}\alpha_iy_i = 0 \\
        0 \le \alpha_i \le C
    \end{cases}
\end{split}\tag{1-1}
$$

其中，$$x_i$$ 表示样本特征<br/>
　　$$y_i$$ 表示样本标签，且 $$y_i \in \{-1,1\}$$ <br/>
　　$$\alpha_i$$ 为要求解的参数 <br/>
　　$$C$$ 为惩罚系数（自己设定）

### SMO 算法原理
　　SMO 算法的基本思想是将原问题求解 $$(\alpha_1,\alpha_2,...,\alpha_N)$$ 这 $$N$$ 个参数的问题分解成多个子二次规划的问题分别求解，每个子问题只需要求解其中的 2 个参数，每次通过启发式选择两个变量进行优化，不断循环，直到达到函数的最优值。

#### 选择 $$\alpha_1$$ $$\alpha_2$$ 为变量
　　将 $$\Psi(\vec{\alpha})$$ 中 $$\alpha_1,\alpha_2$$ 视作变量，其余的 $$\alpha_i i=3,4,...,N$$ 的参数视为常数，则原优化目标函数变换如下：

$$
\begin{split}
\begin{align*}
    min\Psi(\alpha_1,\alpha_2)  =&\frac{1}{2}K_{11}\alpha_{1}^2y_1^2 + \frac{1}{2}K_{22}\alpha_{2}^2y_2^2 \\
   & + \frac12K_{12}\alpha_1\alpha_2y_1y_2 + \frac12K_{21}\alpha_2\alpha_1y_2y_1 \\
   & - (\alpha_1 + \alpha_2)  + y_1v_1\alpha_1 + y_2v_2\alpha_2 + P
\end{align*}
\end{split}\tag{2-1}
$$

由于，$$y_i^2 = 1$$，$$K_{ij}= K_{ji}$$，化简可得：

$$
\begin{split}
\begin{align*}
    min\Psi(\alpha_1,\alpha_2) 
       =&\frac12K_{11}\alpha_1^2 + \frac12K_{22}\alpha_2^2 + K_{12}\alpha_1\alpha_2y_1y_2\\ 
       &-(\alpha_1 + \alpha_2) + y_1v_1\alpha_1 + y_2v_2\alpha_2 + P
\end{align*}
\end{split}\tag{2-2}
$$

其中，$$K_{ij}$$ 为核函数，$$K_{ij} = K(x_i,x_j)$$<br/>
　　$$v_i = \sum\limits_{j=1}^{n} \alpha_iy_jK_{ij} = 0 $$ <br/>
　　$$P$$ 为常数项

#### 用 $$\alpha_2$$ 表示 $$\alpha_1$$

由 $$\sum\limits_{i=1}^{N} \alpha_iy_i = 0$$ 得：

$$
    \alpha_1y_1 + \alpha_2y_2 = - \sum\limits_{i=3}^{N} \alpha_iy_i = \zeta \tag{2-3}
$$

等式两边同时乘 $$y_1$$ 可得：

$$
    \alpha_1 = (\zeta - y_2\alpha_2)y_1 \tag{2-4}
$$

将 (2-4) 代入到 (2-2) 可得：

$$
\begin{align*}
    \Psi(\alpha_2) = &\frac12K_{11}(\zeta - \alpha_2y_2)^2y_1^2 + \frac12K_{22}\alpha_2^2 + K_{12}(\zeta - \alpha_2y_2)\alpha_2y_1^2y_2\\
                    & -(\zeta - \alpha_2y_2)y_1 - \alpha_2 + v_1(\zeta - \alpha_2y_2)y_1^2 + y_2v_2\alpha_2 + P

\end{align*} \tag{2-5}
$$

化简可得：

$$
    \begin{align*}
        \Psi(\alpha_2) = & \frac12K_{11}(\zeta - \alpha_2y_2)^2 + \frac12K_{22}\alpha_2^2 + K_{12}(\zeta - \alpha_2y_2)\alpha_2y_2\\
        & -(\zeta - \alpha_2y_2)y_1 - \alpha_2 + v_1(\zeta - \alpha_2y_2) + y_2v_2\alpha_2 + P
    \end{align*} \tag{2-6}
$$

#### 对 $$\alpha_2$$ 求极值
由 (2-6) 可知，现在目标函数只是关于 $$\alpha_2$$ 的函数，对其求导并令它等于 0 

$$
\begin{align}
    \frac{\partial \Psi (\alpha_2)}{\partial \alpha_2}&=(K_{11}+K_{22}-2K_{12})\alpha_2-K_{11}\zeta y_2+K_{12}\zeta y_2\\
    &\quad+y_1y_2-1-v_1y_2+v_2y_2\\
    & =0
\end{align} \tag{2-7}
$$

SMO 的思想是一个迭代求解的思想，所以必须构造出 $$\alpha_2^{new}$$ 与 $$\alpha_2^{old}$$ 之间的关系，由 (2-3) 可知：

$$
    \alpha_1^{new}y_1 + \alpha_2^{new}y_2 = \alpha_1^{old}y_1 + \alpha_2^{old}y_2 = \zeta \tag{2-8}
$$

由

 $$
 \begin{align*}
    &f(x) =\omega^T x + b  = \sum_{i=1}^{N} \alpha_iy_iK_{ix} + b\\
    &v_i = \sum\limits_{j=3}^{N}\alpha_jy_jK_{ij} \quad\quad i=1,2
 \end{align*} \tag{2-9}
 $$

 可得

 $$
 \begin{align*}
    v_1 = f(x_1) - \sum\limits_{j=1}^{2}\alpha_jy_jK_{1j} - b = f(x) - \alpha_1y_1K_{11} - \alpha_2y_2K_{12} - b \\
    v_2 = f(x_2) - \sum\limits_{j=1}^{2}\alpha_jy_jK_{2j} - b = f(x) - \alpha_1y_1K_{21} - \alpha_2y_2K_{22} - b 
\end{align*} \tag{2-10}
 $$

 将 (2-8), (2-10) 带入到 (2-7) 中得：

 $$
 \begin{align*}
    (K_{11}+K_{22}-2K_{12})\alpha_2^{new,unc}=&(K_{11}+K_{22}-2K_{12})\alpha_2^{old}\\
    & +y_2\left[y_2-y_1+f(x_1)-f(x_2)\right]
\end{align*}\tag{2-11}
 $$

 可得

 $$
    \alpha_2^{new,unc}=\alpha_2^{old}+\frac{y_2(E_1-E_2)}{\eta}
    \tag{2-12}
 $$

其中，$$\alpha_2^{new,unc}$$ 表示未经过约束的计算值，$$\alpha_2^{old}$$ 为上一次的迭代值 <br/>
　　$$E_i = f(x_i) - y_i$$ 表示预测值与真实值的差 <br/>
　　$$\eta=K_{11}+K_{22}-2K_{12} $$ 

#### $$\alpha_2^{new,unc}$$ 的约束
